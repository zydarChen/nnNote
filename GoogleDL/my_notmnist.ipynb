{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## not_mnist\n",
    "- 数据集[notMNIST_large.tar.gz(235.88MB)](http://yaroslavvb.com/upload/notMNIST/notMNIST_large.tar.gz)，[notMNIST_small.tar.gz(8.07MB)](http://yaroslavvb.com/upload/notMNIST/notMNIST_small.tar.gz)\n",
    "- 参考博客[ahangchen](https://github.com/ahangchen/GDLnotes/blob/master/note/lesson-1/practical.md)\n",
    "- 与mnist类似，但不够干净，更接近于真实情况，比mbist任务更难\n",
    "- 从`A`到`J`，共10个classes，28 * 28 image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下载数据集\n",
    "- `download_progress_hook`显示下载进度条\n",
    "- 使用`urllib.urlretrieve`获取远程数据集\n",
    "- 载入文件时可以设置`force`参数，选择是否强制下载\n",
    "- 返回文件路径而不是文件内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified /software/home/chenzh/software/jupyter/nnNote/GoogleDL/data/notMNIST_large.tar.gz\n",
      "Found and verified /software/home/chenzh/software/jupyter/nnNote/GoogleDL/data/notMNIST_small.tar.gz\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from urllib import urlretrieve\n",
    "# 官方教程url需要科学上网\n",
    "# url = 'https://commondatastorage.googleapis.com/books1000/'\n",
    "url = 'http://yaroslavvb.com/upload/notMNIST/'\n",
    "last_percent_reported = None\n",
    "data_root = os.getcwd() + '/data' # Change me to store data elsewhere\n",
    "\n",
    "def download_progress_hook(count, blockSize, totalSize):\n",
    "    \"\"\"\n",
    "    下载进度条\n",
    "    A hook to report the progress of a download. This is mostly intended for users with \n",
    "    slow internet connections. Reports every 5% change in download progress.\n",
    "    \"\"\"\n",
    "    global last_percent_reported\n",
    "    percent = int(count * blockSize * 100 / totalSize)\n",
    "\n",
    "    if last_percent_reported != percent:\n",
    "        if percent % 5 == 0:\n",
    "            sys.stdout.write(\"%s%%\" % percent)\n",
    "            sys.stdout.flush()\n",
    "        else:\n",
    "            sys.stdout.write(\".\")\n",
    "            sys.stdout.flush()\n",
    "      \n",
    "        last_percent_reported = percent\n",
    "        \n",
    "def maybe_download(filename, expected_bytes, force=False):\n",
    "    \"\"\"\n",
    "    Download a file if not present, and make sure it's the right size.\n",
    "    返回文件路径\n",
    "    \"\"\"\n",
    "    dest_filename = os.path.join(data_root, filename)\n",
    "    if force or not os.path.exists(dest_filename):\n",
    "        print('Attempting to download:', filename)\n",
    "        filename, _ = urlretrieve(url + filename, dest_filename, reporthook=download_progress_hook)\n",
    "        print('\\nDownload Complete!')\n",
    "    statinfo = os.stat(dest_filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified', dest_filename)\n",
    "    else:\n",
    "        raise Exception('Failed to verify ' + dest_filename + '. Can you get to it with a browser?')\n",
    "    return dest_filename\n",
    "\n",
    "train_filename = maybe_download('notMNIST_large.tar.gz', 247336696)\n",
    "test_filename = maybe_download('notMNIST_small.tar.gz', 8458043)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解压\n",
    "- 使用tarfile\n",
    "- `os.path.splittext`分离文件名与拓展名\n",
    "- 返回`data_folders`列表，列表内容为`A`到`J`文件夹路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/software/home/chenzh/software/jupyter/nnNote/GoogleDL/data/notMNIST_large already present - Skipping extraction of /software/home/chenzh/software/jupyter/nnNote/GoogleDL/data/notMNIST_large.tar.gz.\n",
      "['/software/home/chenzh/software/jupyter/nnNote/GoogleDL/data/notMNIST_large/A', '/software/home/chenzh/software/jupyter/nnNote/GoogleDL/data/notMNIST_large/B', '/software/home/chenzh/software/jupyter/nnNote/GoogleDL/data/notMNIST_large/C', '/software/home/chenzh/software/jupyter/nnNote/GoogleDL/data/notMNIST_large/D', '/software/home/chenzh/software/jupyter/nnNote/GoogleDL/data/notMNIST_large/E', '/software/home/chenzh/software/jupyter/nnNote/GoogleDL/data/notMNIST_large/F', '/software/home/chenzh/software/jupyter/nnNote/GoogleDL/data/notMNIST_large/G', '/software/home/chenzh/software/jupyter/nnNote/GoogleDL/data/notMNIST_large/H', '/software/home/chenzh/software/jupyter/nnNote/GoogleDL/data/notMNIST_large/I', '/software/home/chenzh/software/jupyter/nnNote/GoogleDL/data/notMNIST_large/J']\n",
      "/software/home/chenzh/software/jupyter/nnNote/GoogleDL/data/notMNIST_small already present - Skipping extraction of /software/home/chenzh/software/jupyter/nnNote/GoogleDL/data/notMNIST_small.tar.gz.\n",
      "['/software/home/chenzh/software/jupyter/nnNote/GoogleDL/data/notMNIST_small/A', '/software/home/chenzh/software/jupyter/nnNote/GoogleDL/data/notMNIST_small/B', '/software/home/chenzh/software/jupyter/nnNote/GoogleDL/data/notMNIST_small/C', '/software/home/chenzh/software/jupyter/nnNote/GoogleDL/data/notMNIST_small/D', '/software/home/chenzh/software/jupyter/nnNote/GoogleDL/data/notMNIST_small/E', '/software/home/chenzh/software/jupyter/nnNote/GoogleDL/data/notMNIST_small/F', '/software/home/chenzh/software/jupyter/nnNote/GoogleDL/data/notMNIST_small/G', '/software/home/chenzh/software/jupyter/nnNote/GoogleDL/data/notMNIST_small/H', '/software/home/chenzh/software/jupyter/nnNote/GoogleDL/data/notMNIST_small/I', '/software/home/chenzh/software/jupyter/nnNote/GoogleDL/data/notMNIST_small/J']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tarfile\n",
    "\n",
    "num_classes = 10\n",
    "np.random.seed(133)\n",
    "\n",
    "def maybe_extract(filename, force=False):\n",
    "    root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "    if os.path.isdir(root) and not force:\n",
    "    # You may override by setting force=True.\n",
    "        print('%s already present - Skipping extraction of %s.' % (root, filename))\n",
    "    else:\n",
    "        print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "        tar = tarfile.open(filename)\n",
    "        sys.stdout.flush()\n",
    "        tar.extractall(data_root)\n",
    "        tar.close()\n",
    "    data_folders = [\n",
    "        os.path.join(root, d) for d in sorted(os.listdir(root)) \n",
    "        if os.path.isdir(os.path.join(root, d))] \n",
    "    if len(data_folders) != num_classes:\n",
    "        raise Exception('Expected %d folders, one per class. Found %d instead.' % (num_classes, len(data_folders)))\n",
    "    print(data_folders)\n",
    "    return data_folders\n",
    "  \n",
    "train_folders = maybe_extract(train_filename)\n",
    "test_folders = maybe_extract(test_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 序列化处理图像\n",
    "- 考虑到内存问题，`load_letter`读取单个字母数据\n",
    "- 使用`scipy.ndimage`读图\n",
    "- 用ndimage读取一部分图片，用pickle将读取到的对象（ndarray对象的list）序列化存储到磁盘\n",
    "- `maybe_pickle`将每个存放字母的文件夹序列化为pickle文件，并存储到本地磁盘，返回路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/software/home/chenzh/software/jupyter/nnNote/GoogleDL/data/notMNIST_large/A.pickle already present - Skipping pickling.\n",
      "/software/home/chenzh/software/jupyter/nnNote/GoogleDL/data/notMNIST_large/B.pickle already present - Skipping pickling.\n",
      "/software/home/chenzh/software/jupyter/nnNote/GoogleDL/data/notMNIST_large/C.pickle already present - Skipping pickling.\n",
      "/software/home/chenzh/software/jupyter/nnNote/GoogleDL/data/notMNIST_large/D.pickle already present - Skipping pickling.\n",
      "/software/home/chenzh/software/jupyter/nnNote/GoogleDL/data/notMNIST_large/E.pickle already present - Skipping pickling.\n",
      "/software/home/chenzh/software/jupyter/nnNote/GoogleDL/data/notMNIST_large/F.pickle already present - Skipping pickling.\n",
      "/software/home/chenzh/software/jupyter/nnNote/GoogleDL/data/notMNIST_large/G.pickle already present - Skipping pickling.\n",
      "/software/home/chenzh/software/jupyter/nnNote/GoogleDL/data/notMNIST_large/H.pickle already present - Skipping pickling.\n",
      "/software/home/chenzh/software/jupyter/nnNote/GoogleDL/data/notMNIST_large/I.pickle already present - Skipping pickling.\n",
      "/software/home/chenzh/software/jupyter/nnNote/GoogleDL/data/notMNIST_large/J.pickle already present - Skipping pickling.\n",
      "/software/home/chenzh/software/jupyter/nnNote/GoogleDL/data/notMNIST_small/A.pickle already present - Skipping pickling.\n",
      "/software/home/chenzh/software/jupyter/nnNote/GoogleDL/data/notMNIST_small/B.pickle already present - Skipping pickling.\n",
      "/software/home/chenzh/software/jupyter/nnNote/GoogleDL/data/notMNIST_small/C.pickle already present - Skipping pickling.\n",
      "/software/home/chenzh/software/jupyter/nnNote/GoogleDL/data/notMNIST_small/D.pickle already present - Skipping pickling.\n",
      "/software/home/chenzh/software/jupyter/nnNote/GoogleDL/data/notMNIST_small/E.pickle already present - Skipping pickling.\n",
      "/software/home/chenzh/software/jupyter/nnNote/GoogleDL/data/notMNIST_small/F.pickle already present - Skipping pickling.\n",
      "/software/home/chenzh/software/jupyter/nnNote/GoogleDL/data/notMNIST_small/G.pickle already present - Skipping pickling.\n",
      "/software/home/chenzh/software/jupyter/nnNote/GoogleDL/data/notMNIST_small/H.pickle already present - Skipping pickling.\n",
      "/software/home/chenzh/software/jupyter/nnNote/GoogleDL/data/notMNIST_small/I.pickle already present - Skipping pickling.\n",
      "/software/home/chenzh/software/jupyter/nnNote/GoogleDL/data/notMNIST_small/J.pickle already present - Skipping pickling.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "image_size = 28  # Pixel width and height.\n",
    "pixel_depth = 255.0  # Number of levels per pixel.\n",
    "\n",
    "def load_letter(folder, min_num_images):\n",
    "    \"\"\"\n",
    "    针对单个字母的文件夹，遍历所有图片并序列化\n",
    "    返回dataset为三维数组\n",
    "    Load the data for a single letter label.\"\"\"\n",
    "    image_files = os.listdir(folder)  # 文件列表\n",
    "    dataset = np.ndarray(shape=(len(image_files), image_size, image_size), dtype=np.float32)\n",
    "    print(folder)\n",
    "    num_images = 0\n",
    "    for image in image_files:  # 每一张图\n",
    "        image_file = os.path.join(folder, image)\n",
    "        try:\n",
    "            image_data = (ndimage.imread(image_file).astype(float) - pixel_depth / 2) / pixel_depth  # 读图\n",
    "            if image_data.shape != (image_size, image_size):\n",
    "                raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "            dataset[num_images, :, :] = image_data\n",
    "            num_images = num_images + 1\n",
    "        except IOError as e:\n",
    "            print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "    \n",
    "    dataset = dataset[0:num_images, :, :]\n",
    "    if num_images < min_num_images:\n",
    "        raise Exception('Many fewer images than expected: %d < %d' % (num_images, min_num_images))\n",
    "    print('Full dataset tensor:', dataset.shape)\n",
    "    print('Mean:', np.mean(dataset))\n",
    "    print('Standard deviation:', np.std(dataset))\n",
    "    return dataset\n",
    "        \n",
    "def maybe_pickle(data_folders, min_num_images_per_class, force=False):\n",
    "    dataset_names = []\n",
    "    for folder in data_folders:\n",
    "        set_filename = folder + '.pickle'\n",
    "        dataset_names.append(set_filename)\n",
    "        if os.path.exists(set_filename) and not force:\n",
    "            # You may override by setting force=True.\n",
    "            print('%s already present - Skipping pickling.' % set_filename)\n",
    "        else:\n",
    "            print('Pickling %s.' % set_filename)\n",
    "            dataset = load_letter(folder, min_num_images_per_class)\n",
    "            try:\n",
    "                with open(set_filename, 'wb') as f:\n",
    "                    pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "            except Exception as e:\n",
    "                print('Unable to save data to', set_filename, ':', e)\n",
    "    return dataset_names\n",
    "                      \n",
    "train_datasets = maybe_pickle(train_folders, 45000)\n",
    "test_datasets = maybe_pickle(test_folders, 1800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 显示图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc00dcd7fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_pickle(pickle_name):\n",
    "    # load a pickle file to memory\n",
    "    if os.path.exists(pickle_name):\n",
    "        return pickle.load(open(pickle_name, \"r\"))\n",
    "    return None\n",
    "\n",
    "def show_imgs(imgs, show_max=-1):\n",
    "    show_cnt = show_max\n",
    "    if show_max == -1:\n",
    "        show_cnt = len(imgs)\n",
    "\n",
    "    for image_index in xrange(show_cnt):\n",
    "        # they are binary images, if RGBs, don't add cmap=\"Graeys\"\n",
    "        plt.imshow(imgs[image_index], cmap=\"Greys\")\n",
    "        plt.show()\n",
    "\n",
    "imgs_A = load_pickle(train_datasets[0])  # 读入A.pickle\n",
    "show_imgs(imgs_A, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练集、测试集、验证集\n",
    "- 训练集200000、测试集10000、验证集10000\n",
    "- `random.shuffle`将数据乱序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (200000, 28, 28) (200000,)\n",
      "Validation: (10000, 28, 28) (10000,)\n",
      "Testing: (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "def make_arrays(nb_rows, img_size):\n",
    "    '''初始化'''\n",
    "    if nb_rows:\n",
    "        dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)\n",
    "        labels = np.ndarray(nb_rows, dtype=np.int32)\n",
    "    else:\n",
    "        dataset, labels = None, None\n",
    "    return dataset, labels\n",
    "\n",
    "def merge_datasets(pickle_files, train_size, valid_size=0):\n",
    "    num_classes = len(pickle_files)\n",
    "    valid_dataset, valid_labels = make_arrays(valid_size, image_size)\n",
    "    train_dataset, train_labels = make_arrays(train_size, image_size)\n",
    "    vsize_per_class = valid_size // num_classes\n",
    "    tsize_per_class = train_size // num_classes\n",
    "    \n",
    "    start_v, start_t = 0, 0\n",
    "    end_v, end_t = vsize_per_class, tsize_per_class\n",
    "    end_l = vsize_per_class+tsize_per_class\n",
    "    for label, pickle_file in enumerate(pickle_files):       \n",
    "        try:\n",
    "            with open(pickle_file, 'rb') as f:\n",
    "                letter_set = pickle.load(f)\n",
    "                # let's shuffle the letters to have random validation and training set\n",
    "                np.random.shuffle(letter_set)\n",
    "                if valid_dataset is not None:\n",
    "                    valid_letter = letter_set[:vsize_per_class, :, :]\n",
    "                    valid_dataset[start_v:end_v, :, :] = valid_letter\n",
    "                    valid_labels[start_v:end_v] = label\n",
    "                    start_v += vsize_per_class\n",
    "                    end_v += vsize_per_class\n",
    "                \n",
    "                train_letter = letter_set[vsize_per_class:end_l, :, :]\n",
    "                train_dataset[start_t:end_t, :, :] = train_letter\n",
    "                train_labels[start_t:end_t] = label\n",
    "                start_t += tsize_per_class\n",
    "                end_t += tsize_per_class\n",
    "        except Exception as e:\n",
    "            print('Unable to process data from', pickle_file, ':', e)\n",
    "            raise\n",
    "            \n",
    "    return valid_dataset, valid_labels, train_dataset, train_labels\n",
    "            \n",
    "            \n",
    "train_size = 200000\n",
    "valid_size = 10000\n",
    "test_size = 10000\n",
    "\n",
    "valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(\n",
    "  train_datasets, train_size, valid_size)\n",
    "_, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size)\n",
    "\n",
    "print('Training:', train_dataset.shape, train_labels.shape)\n",
    "print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "print('Testing:', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正则化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomize(dataset, labels):\n",
    "    permutation = np.random.permutation(labels.shape[0])  # 洗牌\n",
    "    shuffled_dataset = dataset[permutation,:,:]\n",
    "    shuffled_labels = labels[permutation]\n",
    "    return shuffled_dataset, shuffled_labels\n",
    "train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "test_dataset, test_labels = randomize(test_dataset, test_labels)\n",
    "valid_dataset, valid_labels = randomize(valid_dataset, valid_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存到本地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_obj(pickle_file, obj):\n",
    "    try:\n",
    "        f = open(pickle_file, 'wb')\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "        f.close()\n",
    "    except Exception as e:\n",
    "        print('Unable to save data to', pickle_file, ':', e)\n",
    "        raise\n",
    "    statinfo = os.stat(pickle_file)\n",
    "    print('Compressed pickle size:', statinfo.st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed pickle size: 690800441\n"
     ]
    }
   ],
   "source": [
    "pickle_file = os.path.join(data_root, 'notMNIST.pickle')\n",
    "save = {\n",
    "    'train_dataset': train_dataset,\n",
    "    'train_labels': train_labels,\n",
    "    'valid_dataset': valid_dataset,\n",
    "    'valid_labels': valid_labels,\n",
    "    'test_dataset': test_dataset,\n",
    "    'test_labels': test_labels,\n",
    "}\n",
    "save_obj(pickle_file, save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据去重\n",
    "- `.pickle`文件存在重复数据\n",
    "- 每个dataset都是一个二维浮点数组的list，也可以理解为三维浮点数组\n",
    "- 比较list中的每个图，也就是将list1中每个二维浮点数组与list2中每个二维浮点数组比较，但效率极慢\n",
    "- 比较图像的hash值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def img_hash(pix_s):  # hash值计算\n",
    "    seed = 131\n",
    "    v_hash = 0\n",
    "    for row in pix_s:\n",
    "        for p in row:\n",
    "            v_hash = v_hash * seed + int(p * 255)\n",
    "    return v_hash & 0x7FFFFFFF\n",
    "\n",
    "def imgs_idx_hash_except(left, right):\n",
    "    except_idxs = []\n",
    "    print('compute right hash...')\n",
    "    right_hashes = [img_hash(img) for img in tqdm(right)]\n",
    "    print('compare...')\n",
    "    for i in tqdm(range(len(left))):\n",
    "        if img_hash(left[i]) in right_hashes:\n",
    "#             print('compare left[%d] to right found the same' % i)\n",
    "            except_idxs.append(i)\n",
    "    res = np.delete(left, except_idxs, axis=0)\n",
    "    return except_idxs, res\n",
    "\n",
    "def clean():\n",
    "    datasets = load_pickle(os.path.join(data_root , 'notMNIST.pickle'))\n",
    "    test_dataset = datasets['test_dataset']\n",
    "    test_labels = datasets['test_labels']\n",
    "    \n",
    "    print('验证集与测试集去重...')\n",
    "    except_valid_idx, valid_dataset = imgs_idx_hash_except(datasets['valid_dataset'], test_dataset)  # 验证集与测试集去重\n",
    "    valid_labels = np.delete(datasets['valid_labels'], except_valid_idx)\n",
    "    print('训练集与验证集去重...')\n",
    "    except_train_idx, train_dataset = imgs_idx_hash_except(datasets['train_dataset'], valid_dataset)  # 训练集与验证集去重\n",
    "    train_labels = np.delete(datasets['train_labels'], except_train_idx)\n",
    "    print('训练集与测试集去重...')\n",
    "    except_train_idx, train_dataset = imgs_idx_hash_except(train_dataset, test_dataset)  # 训练集与测试集去重\n",
    "    train_labels = np.delete(train_labels, except_train_idx)\n",
    "    \n",
    "    print('train_dataset:%d' % len(train_dataset))\n",
    "    print('train_labels:%d' % len(train_labels))\n",
    "    print('valid_dataset:%d' % len(valid_dataset))\n",
    "    print('valid_labels:%d' % len(valid_labels))\n",
    "    print('test_dataset:%d' % len(test_dataset))\n",
    "    print('test_labels:%d' % len(test_labels))\n",
    "    \n",
    "    pickle_file = os.path.join(data_root, 'notMNIST_clean.pickle')\n",
    "    save = {\n",
    "        'train_dataset': train_dataset,\n",
    "        'train_labels': train_labels,\n",
    "        'valid_dataset': valid_dataset,\n",
    "        'valid_labels': valid_labels,\n",
    "        'test_dataset': test_dataset,\n",
    "        'test_labels': test_labels,\n",
    "    }\n",
    "    save_obj(pickle_file, save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 26/10000 [00:00<00:39, 255.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute right hash...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:39<00:00, 251.82it/s]\n",
      "  0%|          | 25/10000 [00:00<00:41, 241.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compare...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:42<00:00, 235.26it/s]\n",
      "  0%|          | 26/9828 [00:00<00:38, 254.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute right hash...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9828/9828 [00:38<00:00, 255.27it/s]\n",
      "  0%|          | 24/200000 [00:00<14:02, 237.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compare...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200000/200000 [14:14<00:00, 234.18it/s]\n",
      "  0%|          | 26/10000 [00:00<00:38, 257.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute right hash...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:40<00:00, 248.47it/s]\n",
      "  0%|          | 24/198673 [00:00<14:02, 235.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compare...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 198673/198673 [14:20<00:00, 230.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset:195151\n",
      "train_labels:195151\n",
      "valid_dataset:9828\n",
      "valid_labels:9828\n",
      "test_dataset:10000\n",
      "test_labels:10000\n",
      "Compressed pickle size: 675034501\n"
     ]
    }
   ],
   "source": [
    "clean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练一个logistics 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from six.moves import cPickle as pickle\n",
    "data_root = os.getcwd() + '/data' # Change me to store data elsewhere\n",
    "\n",
    "def load_pickle(pickle_name):\n",
    "    # load a pickle file to memory\n",
    "    if os.path.exists(pickle_name):\n",
    "        return pickle.load(open(pickle_name, \"r\"))\n",
    "    return None\n",
    "\n",
    "def save_obj(pickle_file, obj):\n",
    "    try:\n",
    "        f = open(pickle_file, 'wb')\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "        f.close()\n",
    "    except Exception as e:\n",
    "        print('Unable to save data to', pickle_file, ':', e)\n",
    "        raise\n",
    "    statinfo = os.stat(pickle_file)\n",
    "    print('Compressed pickle size:', statinfo.st_size)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "datasets = load_pickle(os.path.join(data_root, 'notMNIST_clean.pickle'))\n",
    "train_dataset = datasets['train_dataset']\n",
    "train_labels = datasets['train_labels']\n",
    "valid_dataset = datasets['valid_dataset']\n",
    "valid_labels = datasets['valid_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 3 1 5 7 0 0 2 5 1 6 0 3 8]\n",
      "[4 3 1 5 7 6 0 2 5 1 6 0 3 8]\n",
      "success rate:82.3768823769%\n"
     ]
    }
   ],
   "source": [
    "classifier_name = os.path.join(data_root, 'classifier.pickle')\n",
    "\n",
    "if os.path.exists(classifier_name):\n",
    "    classifier = load_pickle(classifier_name)\n",
    "else:\n",
    "    classifier = LogisticRegression()\n",
    "    classifier.fit(train_dataset.reshape(train_dataset.shape[0], -1), train_labels)\n",
    "    save_obj(classifier_name, classifier)\n",
    "\n",
    "# simple valid\n",
    "valid_idx_s = 3000\n",
    "valid_idx_e = 3014\n",
    "x = classifier.predict(valid_dataset.reshape(valid_dataset.shape[0], -1)[valid_idx_s: valid_idx_e])\n",
    "print(x)\n",
    "print(valid_labels[valid_idx_s:valid_idx_e])\n",
    "\n",
    "# whole valid\n",
    "x = classifier.predict(valid_dataset.reshape(valid_dataset.shape[0], -1))\n",
    "fail_cnt = 0\n",
    "for i, pred in enumerate(x):\n",
    "    if pred != valid_labels[i]:\n",
    "        fail_cnt += 1\n",
    "print(\"success rate:\" + str((1 - float(fail_cnt) / len(x)) * 100) + \"%\")\n",
    "\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# print ('accuracy:' + accuracy_score(valid_labels, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
